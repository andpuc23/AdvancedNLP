{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\snipercapt\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\snipercapt\\Desktop\\ANLP\\AdvancedNLP\\Task_3\\code_\\bert.py:10: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric(\"seqeval\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from code_.process_conll import process_file, advanced_process_file\n",
    "from code_.evaluation import class_report_base, class_report_advanced, shrink_predictions\n",
    "from code_.bert import Tokenizer, convert_to_dataset, get_labels_list_from_dataset\n",
    "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "from datasets import Dataset, load_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_list = ['ARG0', 'ARG1', 'ARG1-DSP', 'ARG2', 'ARG3', 'ARG4', 'ARG5', 'ARGA', 'ARGM-ADJ', 'ARGM-ADV', 'ARGM-CAU', 'ARGM-COM', 'ARGM-CXN', 'ARGM-DIR', 'ARGM-DIS', 'ARGM-EXT', 'ARGM-GOL', 'ARGM-LOC', 'ARGM-LVB', 'ARGM-MNR', 'ARGM-MOD', 'ARGM-NEG', 'ARGM-PRD', 'ARGM-PRP', 'ARGM-PRR', 'ARGM-REC', 'ARGM-TMP', 'C-ARG0', 'C-ARG1', 'C-ARG1-DSP', 'C-ARG2', 'C-ARG3', 'C-ARG4', 'C-ARGM-ADV', 'C-ARGM-COM', 'C-ARGM-CXN', 'C-ARGM-DIR', 'C-ARGM-EXT', 'C-ARGM-GOL', 'C-ARGM-LOC', 'C-ARGM-MNR', 'C-ARGM-PRP', 'C-ARGM-PRR', 'C-ARGM-TMP', 'C-V', 'R-ARG0', 'R-ARG1', 'R-ARG2', 'R-ARG3', 'R-ARG4', 'R-ARGM-ADJ', 'R-ARGM-ADV', 'R-ARGM-CAU', 'R-ARGM-COM', 'R-ARGM-DIR', 'R-ARGM-GOL', 'R-ARGM-LOC', 'R-ARGM-MNR', 'R-ARGM-TMP', 'V', '_']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = load_metric(\"seqeval\")\n",
    "\n",
    "def compute_metrics(p):\n",
    "\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    # Remove ignored index (special tokens)\n",
    "    true_predictions = [\n",
    "        [labels_list[p] for (p, l) in zip(prediction, label) if l != -100 and p < len(labels_list)]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [labels_list[l] for (p, l) in zip(prediction, label) if l != -100 and p < len(labels_list)]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "\n",
    "    return {\n",
    "            \"precision\": results[\"overall_precision\"],\n",
    "            \"recall\": results[\"overall_recall\"],\n",
    "            \"f1\": results[\"overall_f1\"],\n",
    "            \"accuracy\": results[\"overall_accuracy\"],\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(\"distilbert-base-uncased\", labels_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained('model_checkpoints/baseline', num_labels=len(labels_list))\n",
    "# model = AutoModelForTokenClassification.from_pretrained('model_checkpoints/advanced', num_labels=len(labels_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\snipercapt\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\accelerate\\accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model_name = \"distilbert-base-uncased\"\n",
    "args = TrainingArguments(\n",
    "    \"model_checkpoints/baseline\",\n",
    "    evaluation_strategy = 'epoch',\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    push_to_hub=False,\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model, args,\n",
    "    # train_dataset=tokenized_datasets[\"train\"],\n",
    "    # eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=DataCollatorForTokenClassification(tokenizer.tokenizer, padding=True),\n",
    "    tokenizer=tokenizer.tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(tok:Tokenizer, trainer:Trainer, examples:list, use_context:bool, save_file:str):\n",
    "    if not use_context:\n",
    "        tokenized_data = tok.tokenize_and_align_labels_pred(examples)\n",
    "    else: \n",
    "        tokenized_data = tok.tokenize_and_align_labels_context(examples)\n",
    "    # todo context\n",
    "\n",
    "    dataset = Dataset.from_dict(tokenized_data)\n",
    "\n",
    "\n",
    "    predictions_raw, labels_pred, _ = trainer.predict(dataset)\n",
    "\n",
    "    predictions = np.argmax(predictions_raw, axis=2)\n",
    "\n",
    "    list_predictions = [\n",
    "        [labels_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels_pred)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [labels_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels_pred)\n",
    "    ]\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    val_word_ids = [tok.tokenizer(sentence, truncation=False, is_split_into_words=True).word_ids() for sentence in examples['sentence']]\n",
    "\n",
    "    df = pd.DataFrame(columns=['sentence', 'prediction', 'gold', 'word_ids'])\n",
    "    for tokens, prediction, gold, word_ids in zip(tokenized_data['input_ids'], list_predictions, true_labels, val_word_ids):\n",
    "        sentence = tok.tokenizer.decode(tokens)\n",
    "        df.loc[len(df.index)] = [sentence, prediction, gold, word_ids]\n",
    "\n",
    "    gold_restored = []\n",
    "    pred_restored = []\n",
    "    for i, row in df.iterrows():\n",
    "        sentence = row[0]\n",
    "        orig_sentence = sentence.split('[SEP]')[0].split(' ')[1:]\n",
    "        prediction = row[1]\n",
    "        gold = row[2]\n",
    "        word_ids = row[3][1:-1]\n",
    "        gold_restored.append(shrink_predictions(word_ids, gold))\n",
    "        pred_restored.append(shrink_predictions(word_ids, prediction))\n",
    "\n",
    "    df['gold_restored'] = gold_restored\n",
    "    df['pred_restored'] = pred_restored\n",
    "\n",
    "    print(df.columns)\n",
    "    df.to_csv(save_file)\n",
    "    class_report_base(save_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\snipercapt\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: ARG0 seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "c:\\Users\\snipercapt\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: V seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "c:\\Users\\snipercapt\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: _ seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "c:\\Users\\snipercapt\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: ARG1 seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "c:\\Users\\snipercapt\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\seqeval\\metrics\\v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\snipercapt\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\seqeval\\metrics\\v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 62.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['sentence', 'prediction', 'gold', 'word_ids', 'gold_restored',\n",
      "       'pred_restored'],\n",
      "      dtype='object')\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         'V'       1.00      1.00      1.00         2\n",
      "         '_'       0.00      0.00      0.00         2\n",
      "       'C-V'       0.00      0.00      0.00         0\n",
      "      'ARGA'       0.00      0.00      0.00         0\n",
      "      'ARG3'       0.00      0.00      0.00         0\n",
      "      'ARG2'       0.00      0.00      0.00         0\n",
      "      'ARG5'       0.00      0.00      0.00         0\n",
      "      'ARG0'       0.00      0.00      0.00         2\n",
      "      'ARG4'       0.00      0.00      0.00         0\n",
      "      'ARG1'       0.00      0.00      0.00         0\n",
      "    'C-ARG4'       0.00      0.00      0.00         0\n",
      "    'C-ARG2'       0.00      0.00      0.00         0\n",
      "    'R-ARG2'       0.00      0.00      0.00         0\n",
      "    'C-ARG0'       0.00      0.00      0.00         0\n",
      "    'R-ARG0'       0.00      0.00      0.00         0\n",
      "    'C-ARG1'       0.00      0.00      0.00         0\n",
      "    'C-ARG3'       0.00      0.00      0.00         0\n",
      "    'R-ARG3'       0.00      0.00      0.00         0\n",
      "    'R-ARG1'       0.00      0.00      0.00         0\n",
      "  'ARGM-EXT'       0.00      0.00      0.00         0\n",
      "  'ARGM-NEG'       0.00      0.00      0.00         0\n",
      "  'ARGM-DIS'       0.00      0.00      0.00         0\n",
      "  'ARGM-PRD'       0.00      0.00      0.00         0\n",
      "  'ARGM-PRP'       0.00      0.00      0.00         0\n",
      "  'ARGM-ADV'       0.00      0.00      0.00         0\n",
      "  'ARGM-GOL'       0.00      0.00      0.00         0\n",
      "  'ARGM-REC'       0.00      0.00      0.00         0\n",
      "  'ARG1-DSP'       0.00      0.00      0.00         0\n",
      "  'ARGM-MNR'       0.00      0.00      0.00         0\n",
      "  'ARGM-PRR'       0.00      0.00      0.00         0\n",
      "  'ARGM-DIR'       0.00      0.00      0.00         0\n",
      "  'ARGM-LVB'       0.00      0.00      0.00         0\n",
      "  'ARGM-TMP'       0.00      0.00      0.00         0\n",
      "  'ARGM-ADJ'       0.00      0.00      0.00         0\n",
      "  'ARGM-MOD'       0.00      0.00      0.00         0\n",
      "  'ARGM-COM'       0.00      0.00      0.00         0\n",
      "  'ARGM-LOC'       0.00      0.00      0.00         0\n",
      "  'ARGM-CAU'       0.00      0.00      0.00         0\n",
      "  'ARGM-CXN'       0.00      0.00      0.00         0\n",
      "'C-ARG1-DSP'       0.00      0.00      0.00         0\n",
      "'R-ARGM-DIR'       0.00      0.00      0.00         0\n",
      "'R-ARGM-LOC'       0.00      0.00      0.00         0\n",
      "'R-ARGM-ADV'       0.00      0.00      0.00         0\n",
      "'R-ARGM-MNR'       0.00      0.00      0.00         0\n",
      "'C-ARGM-EXT'       0.00      0.00      0.00         0\n",
      "'C-ARGM-LOC'       0.00      0.00      0.00         0\n",
      "'C-ARGM-CXN'       0.00      0.00      0.00         0\n",
      "'R-ARGM-ADJ'       0.00      0.00      0.00         0\n",
      "'R-ARGM-TMP'       0.00      0.00      0.00         0\n",
      "'R-ARGM-CAU'       0.00      0.00      0.00         0\n",
      "'C-ARGM-MNR'       0.00      0.00      0.00         0\n",
      "'R-ARGM-COM'       0.00      0.00      0.00         0\n",
      "\n",
      "   micro avg       0.33      0.33      0.33         6\n",
      "   macro avg       0.02      0.02      0.02         6\n",
      "weighted avg       0.33      0.33      0.33         6\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "c:\\Users\\snipercapt\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\snipercapt\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\snipercapt\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\snipercapt\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\snipercapt\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\snipercapt\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "examples = {\n",
    "    'sentence': [['some','fucking','sentence'], ['another','fucking','sentence']],\n",
    "    'predicate': ['fucking', 'fucking'],\n",
    "    'labels': ['ARG0, V, _', 'ARG0, V, _']\n",
    "}\n",
    "\n",
    "\n",
    "run_model(tokenizer,\n",
    "          trainer,\n",
    "          examples,\n",
    "          use_context=False, \n",
    "          save_file='data/challenge_results/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
