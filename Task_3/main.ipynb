{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "U6bQle4zH0TW"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n",
        "from transformers import DataCollatorForTokenClassification\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from code_.bert import convert_to_dataset, get_labels_list_from_dataset, Tokenizer, compute_metrics, load_metric\n",
        "from code_.process_conll import process_file, advanced_process_file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ErB26AmPGexG"
      },
      "outputs": [],
      "source": [
        "task = \"ner\" # Should be one of \"ner\", \"pos\" or \"chunk\"\n",
        "model_checkpoint = \"distilbert-base-uncased\"\n",
        "batch_size = 32"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "WUvyO_ZfGexG"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "advanced_process_file(): dataframe len: 4979\n",
            "advanced_process_file(): dataframe len: 40498\n",
            "advanced_process_file(): dataframe len: 4802\n"
          ]
        }
      ],
      "source": [
        "df_val = advanced_process_file('data/raw/en_ewt-up-dev.conllu')\n",
        "df_train = advanced_process_file('data/raw/en_ewt-up-train.conllu')\n",
        "df_test = advanced_process_file('data/raw/en_ewt-up-test.conllu')\n",
        "\n",
        "# df_val = process_file('/content/en_ewt-up-dev.conllu')\n",
        "# df_train = process_file('/content/en_ewt-up-train.conllu')\n",
        "# df_test = process_file('/content/en_ewt-up-test.conllu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "fGeIlh_RGexI"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['ARG0', 'ARG1', 'ARG1-DSP', 'ARG2', 'ARG3', 'ARG4', 'ARG5', 'ARGA', 'ARGM-ADJ', 'ARGM-ADV', 'ARGM-CAU', 'ARGM-COM', 'ARGM-CXN', 'ARGM-DIR', 'ARGM-DIS', 'ARGM-EXT', 'ARGM-GOL', 'ARGM-LOC', 'ARGM-LVB', 'ARGM-MNR', 'ARGM-MOD', 'ARGM-NEG', 'ARGM-PRD', 'ARGM-PRP', 'ARGM-PRR', 'ARGM-REC', 'ARGM-TMP', 'C-ARG0', 'C-ARG1', 'C-ARG1-DSP', 'C-ARG2', 'C-ARG3', 'C-ARG4', 'C-ARGM-ADV', 'C-ARGM-COM', 'C-ARGM-CXN', 'C-ARGM-DIR', 'C-ARGM-EXT', 'C-ARGM-GOL', 'C-ARGM-LOC', 'C-ARGM-MNR', 'C-ARGM-PRP', 'C-ARGM-PRR', 'C-ARGM-TMP', 'C-V', 'R-ARG0', 'R-ARG1', 'R-ARG2', 'R-ARG3', 'R-ARG4', 'R-ARGM-ADJ', 'R-ARGM-ADV', 'R-ARGM-CAU', 'R-ARGM-COM', 'R-ARGM-DIR', 'R-ARGM-GOL', 'R-ARGM-LOC', 'R-ARGM-MNR', 'R-ARGM-TMP', 'V', '_']\n"
          ]
        }
      ],
      "source": [
        "dataset = convert_to_dataset(df_train, df_val, df_test)\n",
        "labels_list = get_labels_list_from_dataset(dataset)\n",
        "print(sorted(labels_list))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "oO33ERDPGexI"
      },
      "outputs": [],
      "source": [
        "tok = Tokenizer(model_checkpoint, labels_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "qqy-HHS2GexJ"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1877cdbb94724b34a9d7666319ddd898",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/40498 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "68f52bad496c4c2d9cb3589b5902641a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/4979 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6dc9bb8757214d9f9d41606693c6cd8c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/4802 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "tokenized_datasets = dataset.map(tok.tokenize_and_align_labels_context, batched=True)\n",
        "# tokenized_datasets = dataset.map(tok.tokenize_and_align_labels_pred, batched=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "z0bf8otVGexJ"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "model = AutoModelForTokenClassification.from_pretrained(model_checkpoint, num_labels=len(labels_list))\n",
        "# model = AutoModelForTokenClassification.from_pretrained('model_checkpoints/pred', num_labels=len(labels_list))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "IZgrSP1xGexK"
      },
      "outputs": [],
      "source": [
        "model_name = model_checkpoint.split(\"/\")[-1]\n",
        "args = TrainingArguments(\n",
        "    f\"model_checkpoints/pred\",\n",
        "    evaluation_strategy = 'epoch',\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    num_train_epochs=1,\n",
        "    weight_decay=0.01,\n",
        "    push_to_hub=False,\n",
        ")\n",
        "\n",
        "data_collator = DataCollatorForTokenClassification(tok.tokenizer)\n",
        "metric = load_metric(\"seqeval\")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model,\n",
        "    args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation\"],\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tok.tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "veXQ31mxGexL"
      },
      "outputs": [],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VX7gKfH0IbBZ"
      },
      "outputs": [],
      "source": [
        "trainer.evaluate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HexqnT2uGexN"
      },
      "outputs": [],
      "source": [
        "predictions_raw, labels, _ = trainer.predict(tokenized_datasets[\"validation\"])\n",
        "predictions = np.argmax(predictions_raw, axis=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "xQldgMFYKson"
      },
      "outputs": [],
      "source": [
        "true_predictions = [\n",
        "    [labels_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
        "    for prediction, label in zip(predictions, labels)\n",
        "]\n",
        "true_labels = [\n",
        "    [labels_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
        "    for prediction, label in zip(predictions, labels)\n",
        "]\n",
        "\n",
        "results = metric.compute(predictions=true_predictions, references=true_labels)\n",
        "results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-9gdt6tyLIk_"
      },
      "outputs": [],
      "source": [
        "trainer.save_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LDcH_UtiXi3G"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nXQLIwAcLvKT"
      },
      "outputs": [],
      "source": [
        "df = pd.DataFrame(columns=['sentence', 'prediction', 'gold'])\n",
        "for tokens, prediction, gold in zip(tokenized_datasets['validation']['input_ids'], true_predictions, true_labels):\n",
        "    sentence = tok.tokenizer.decode(tokens)\n",
        "    df.loc[len(df.index)] = [sentence, prediction, gold]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7mTV5Y1gWmlf"
      },
      "outputs": [],
      "source": [
        "df.to_csv('base.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-8B_RWEVYwOe"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "498zYjPvZvUC"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "21634ba1c762bc3505d86a0c5bab9f12f9ed8aa21c75b1f37c314fb51f344c98"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
