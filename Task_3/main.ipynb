{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55a7dead-418c-4437-b8b6-902338de0f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9ab22f1-b412-49f5-b06a-550c103af9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from code_.process_conll import process_file, advanced_process_file\n",
    "from code_.evaluation import class_report_base, class_report_advanced, shrink_predictions\n",
    "from code_.bert import Tokenizer, convert_to_dataset, compute_metrics, get_labels_list_from_dataset, task, batch_size, model_checkpoint\n",
    "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "from datasets import load_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf75396b-520a-4cc5-b3e4-3ff807a8d367",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "process_file(): dataframe len: 4979\n",
      "process_file(): dataframe len: 40498\n",
      "process_file(): dataframe len: 4802\n"
     ]
    }
   ],
   "source": [
    "df_val = process_file('data/raw/en_ewt-up-dev.conllu')\n",
    "df_train = process_file('data/raw/en_ewt-up-train.conllu')\n",
    "df_test = process_file('data/raw/en_ewt-up-test.conllu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8365c44c-81aa-484a-8461-55e67f2811d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ARG0', 'ARG1', 'ARG1-DSP', 'ARG2', 'ARG3', 'ARG4', 'ARG5', 'ARGA', 'ARGM-ADJ', 'ARGM-ADV', 'ARGM-CAU', 'ARGM-COM', 'ARGM-CXN', 'ARGM-DIR', 'ARGM-DIS', 'ARGM-EXT', 'ARGM-GOL', 'ARGM-LOC', 'ARGM-LVB', 'ARGM-MNR', 'ARGM-MOD', 'ARGM-NEG', 'ARGM-PRD', 'ARGM-PRP', 'ARGM-PRR', 'ARGM-REC', 'ARGM-TMP', 'C-ARG0', 'C-ARG1', 'C-ARG1-DSP', 'C-ARG2', 'C-ARG3', 'C-ARG4', 'C-ARGM-ADV', 'C-ARGM-COM', 'C-ARGM-CXN', 'C-ARGM-DIR', 'C-ARGM-EXT', 'C-ARGM-GOL', 'C-ARGM-LOC', 'C-ARGM-MNR', 'C-ARGM-PRP', 'C-ARGM-PRR', 'C-ARGM-TMP', 'R-ARG0', 'R-ARG1', 'R-ARG2', 'R-ARG3', 'R-ARG4', 'R-ARGM-ADJ', 'R-ARGM-ADV', 'R-ARGM-CAU', 'R-ARGM-COM', 'R-ARGM-DIR', 'R-ARGM-GOL', 'R-ARGM-LOC', 'R-ARGM-MNR', 'R-ARGM-TMP', '_']\n"
     ]
    }
   ],
   "source": [
    "dataset = convert_to_dataset(df_train, df_val, df_test)\n",
    "labels_list = get_labels_list_from_dataset(dataset)\n",
    "print(labels_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b9ad125-2e36-4a67-a04f-95c273e42ac3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdda8cf8cc4645a5a5a067468b550966",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/40498 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80833db7864f4922928f6d62746cde14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4979 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "952fc85ecbe3433e8c228e13e350e898",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4802 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tok = Tokenizer(model_checkpoint, labels_list)\n",
    "tokenized_datasets = dataset.map(tok.tokenize_and_align_labels_pred, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef05046c-4b1c-4baa-bbf1-287c08b77cd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained(model_checkpoint, num_labels=len(labels_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24769bcc-8ed5-4d6f-8a98-41fd94e5cfbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "args = TrainingArguments(\n",
    "    f\"model_checkpoints/baseline\",\n",
    "    evaluation_strategy = 'epoch',\n",
    "    save_steps=1000,\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tok.tokenizer)\n",
    "metric = load_metric(\"seqeval\")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tok.tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ef71c8b-d814-4e3b-a956-826c05a9428e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3165' max='3165' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3165/3165 17:01, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.346800</td>\n",
       "      <td>0.156694</td>\n",
       "      <td>0.647999</td>\n",
       "      <td>0.621218</td>\n",
       "      <td>0.634326</td>\n",
       "      <td>0.958976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.132100</td>\n",
       "      <td>0.129621</td>\n",
       "      <td>0.679661</td>\n",
       "      <td>0.707094</td>\n",
       "      <td>0.693106</td>\n",
       "      <td>0.964137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.107500</td>\n",
       "      <td>0.119227</td>\n",
       "      <td>0.703613</td>\n",
       "      <td>0.722093</td>\n",
       "      <td>0.712734</td>\n",
       "      <td>0.966666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.084500</td>\n",
       "      <td>0.115446</td>\n",
       "      <td>0.712812</td>\n",
       "      <td>0.729561</td>\n",
       "      <td>0.721089</td>\n",
       "      <td>0.967707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.078500</td>\n",
       "      <td>0.113801</td>\n",
       "      <td>0.718075</td>\n",
       "      <td>0.731943</td>\n",
       "      <td>0.724943</td>\n",
       "      <td>0.968094</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()\n",
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "41ad79b0-922d-44f5-acff-a9a831820a48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# predictions_raw, labels, _ = trainer.predict(tokenized_datasets[\"test\"])\n",
    "predictions_raw, labels, _ = trainer.predict(tokenized_datasets[\"validation\"])\n",
    "predictions = np.argmax(predictions_raw, axis=2)\n",
    "\n",
    "true_predictions = [\n",
    "    [labels_list[p] for (p, l) in zip(prediction, label) if l != -100 and p < len(labels_list)]\n",
    "    for prediction, label in zip(predictions, labels)\n",
    "]\n",
    "true_labels = [\n",
    "    [labels_list[l] for (p, l) in zip(prediction, label) if l != -100 and p < len(labels_list)]\n",
    "    for prediction, label in zip(predictions, labels)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "66acc846-ffbf-4bb8-bc3f-02d3f5f55472",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_word_ids = []\n",
    "for sentence in dataset['validation']['sentence']:\n",
    "# for sentence in dataset['test']['sentence']:\n",
    "    val_word_ids.append(tok.tokenizer(sentence, truncation=True, is_split_into_words=True).word_ids())\n",
    "\n",
    "df = pd.DataFrame(columns=['sentence', 'prediction', 'gold', 'word_ids'])\n",
    "for tokens, prediction, gold, word_ids in zip(tokenized_datasets['validation']['input_ids'], true_predictions, true_labels, val_word_ids):\n",
    "# for tokens, prediction, gold, word_ids in zip(tokenized_datasets['test']['input_ids'], true_predictions, true_labels, val_word_ids):\n",
    "    sentence = tok.tokenizer.decode(tokens)\n",
    "    df.loc[len(df.index)] = [sentence, prediction, gold, word_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "52f800f9-102b-4deb-8232-5e2275d885b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_restored = []\n",
    "pred_restored = []\n",
    "for i, row in df.iterrows():\n",
    "    sentence = row[0]\n",
    "    orig_sentence = sentence.split('[SEP]')[0].split(' ')[1:]\n",
    "    prediction = row[1]\n",
    "    gold = row[2]\n",
    "    word_ids = row[3][1:-1]\n",
    "    gold_restored.append(shrink_predictions(word_ids, gold))\n",
    "    pred_restored.append(shrink_predictions(word_ids, prediction))\n",
    "\n",
    "df['gold_restored'] = gold_restored\n",
    "df['pred_restored'] = pred_restored\n",
    "df.to_csv('data/output/base_val.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7b848a50-83b0-47c9-8eb6-feee04743609",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         'V'       0.00      0.00      0.00         0\n",
      "         '_'       0.99      0.99      0.99     95623\n",
      "       'C-V'       0.00      0.00      0.00         0\n",
      "      'ARGA'       0.00      0.00      0.00         0\n",
      "      'ARG3'       0.00      0.00      0.00        82\n",
      "      'ARG2'       0.71      0.73      0.72      1298\n",
      "      'ARG5'       0.00      0.00      0.00         1\n",
      "      'ARG0'       0.81      0.85      0.83      1389\n",
      "      'ARG4'       0.51      0.33      0.40        54\n",
      "      'ARG1'       0.79      0.86      0.82      3218\n",
      "    'C-ARG4'       0.00      0.00      0.00         0\n",
      "    'C-ARG2'       0.00      0.00      0.00         7\n",
      "    'R-ARG2'       0.00      0.00      0.00         4\n",
      "    'C-ARG0'       0.00      0.00      0.00         5\n",
      "    'R-ARG0'       0.79      0.85      0.82        54\n",
      "    'C-ARG1'       0.50      0.33      0.40        55\n",
      "    'C-ARG3'       0.00      0.00      0.00         7\n",
      "    'R-ARG3'       0.00      0.00      0.00         1\n",
      "    'R-ARG1'       0.68      0.72      0.70        65\n",
      "  'ARGM-EXT'       0.74      0.71      0.73       114\n",
      "  'ARGM-NEG'       0.89      0.92      0.91       305\n",
      "  'ARGM-DIS'       0.76      0.53      0.62        97\n",
      "  'ARGM-PRD'       0.29      0.04      0.07        53\n",
      "  'ARGM-PRP'       0.47      0.60      0.53        63\n",
      "  'ARGM-ADV'       0.65      0.57      0.61       454\n",
      "  'ARGM-GOL'       0.00      0.00      0.00        28\n",
      "  'ARGM-REC'       0.00      0.00      0.00         4\n",
      "  'ARG1-DSP'       0.00      0.00      0.00         0\n",
      "  'ARGM-MNR'       0.51      0.46      0.48       169\n",
      "  'ARGM-PRR'       0.69      0.71      0.70        77\n",
      "  'ARGM-DIR'       0.47      0.40      0.43        52\n",
      "  'ARGM-LVB'       0.77      0.82      0.79        66\n",
      "  'ARGM-TMP'       0.77      0.83      0.80       540\n",
      "  'ARGM-ADJ'       0.69      0.69      0.69       235\n",
      "  'ARGM-MOD'       0.92      0.95      0.93       368\n",
      "  'ARGM-COM'       0.00      0.00      0.00        15\n",
      "  'ARGM-LOC'       0.50      0.59      0.54       276\n",
      "  'ARGM-CAU'       0.55      0.48      0.51        62\n",
      "  'ARGM-CXN'       1.00      0.14      0.25        14\n",
      "'C-ARG1-DSP'       0.00      0.00      0.00         0\n",
      "'R-ARGM-DIR'       0.00      0.00      0.00         0\n",
      "'R-ARGM-LOC'       0.50      0.20      0.29        10\n",
      "'R-ARGM-ADV'       0.00      0.00      0.00         1\n",
      "'R-ARGM-MNR'       0.00      0.00      0.00         2\n",
      "'C-ARGM-EXT'       0.00      0.00      0.00         2\n",
      "'C-ARGM-LOC'       0.00      0.00      0.00         3\n",
      "'C-ARGM-CXN'       0.00      0.00      0.00         8\n",
      "'R-ARGM-ADJ'       0.00      0.00      0.00         0\n",
      "'R-ARGM-TMP'       0.00      0.00      0.00         8\n",
      "'R-ARGM-CAU'       0.00      0.00      0.00         1\n",
      "'C-ARGM-MNR'       0.00      0.00      0.00         1\n",
      "'R-ARGM-COM'       0.00      0.00      0.00         1\n",
      "\n",
      "   micro avg       0.97      0.97      0.97    104892\n",
      "   macro avg       0.33      0.29      0.30    104892\n",
      "weighted avg       0.97      0.97      0.97    104892\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from code_.evaluation import class_report_base\n",
    "class_report_base('data/output/base_val.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7218ce5-27f3-4ee0-bb3d-c05d1fe41175",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0778b963-4089-4ffd-bd91-ef409ed19f26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9dc214-c5ba-489a-9968-0a46fac90b17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb87995-13cc-4579-8fc3-69db870ca4c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874b19e3-c6b0-45b2-807e-457a2b19bf58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3bc133-baab-4621-8c81-74f5a4d76801",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebfd34f3-7c83-46e6-a821-9b084de54a52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec57cd10-b08d-4607-9abe-8635dcee8e85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ffb8ae-44f8-4f43-a230-d37782441e8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bca5b108-8028-4a74-b4c5-4a7a14759c63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "advanced_process_file(): dataframe len: 4979\n",
      "advanced_process_file(): dataframe len: 40498\n",
      "advanced_process_file(): dataframe len: 4802\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be59fbf1b7354ea294d290e1737604b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/40498 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb7d33c430ea4462acad620700c0fe3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4979 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9213ebf155454c128cde12beab136fda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4802 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3165' max='3165' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3165/3165 17:22, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.326500</td>\n",
       "      <td>0.138324</td>\n",
       "      <td>0.663766</td>\n",
       "      <td>0.643299</td>\n",
       "      <td>0.653372</td>\n",
       "      <td>0.964538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.119400</td>\n",
       "      <td>0.111597</td>\n",
       "      <td>0.705745</td>\n",
       "      <td>0.717330</td>\n",
       "      <td>0.711490</td>\n",
       "      <td>0.969759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.095000</td>\n",
       "      <td>0.102366</td>\n",
       "      <td>0.723934</td>\n",
       "      <td>0.741084</td>\n",
       "      <td>0.732409</td>\n",
       "      <td>0.972047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.073100</td>\n",
       "      <td>0.098756</td>\n",
       "      <td>0.735281</td>\n",
       "      <td>0.746878</td>\n",
       "      <td>0.741034</td>\n",
       "      <td>0.973093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.067600</td>\n",
       "      <td>0.098428</td>\n",
       "      <td>0.733363</td>\n",
       "      <td>0.749839</td>\n",
       "      <td>0.741509</td>\n",
       "      <td>0.973045</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_val = advanced_process_file('data/raw/en_ewt-up-dev.conllu')\n",
    "df_train = advanced_process_file('data/raw/en_ewt-up-train.conllu')\n",
    "df_test = advanced_process_file('data/raw/en_ewt-up-test.conllu')\n",
    "\n",
    "dataset = convert_to_dataset(df_train, df_val, df_test)\n",
    "labels_list = get_labels_list_from_dataset(dataset)\n",
    "\n",
    "tok = Tokenizer(model_checkpoint, labels_list)\n",
    "tokenized_datasets = dataset.map(tok.tokenize_and_align_labels_context, batched=True)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_checkpoint, num_labels=len(labels_list))\n",
    "\n",
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "args = TrainingArguments(\n",
    "    f\"model_checkpoints/context\",\n",
    "    evaluation_strategy = 'epoch',\n",
    "    learning_rate=2e-5,\n",
    "    save_steps=7000,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    push_to_hub=False,\n",
    ")\n",
    "data_collator = DataCollatorForTokenClassification(tok.tokenizer)\n",
    "metric = load_metric(\"seqeval\")\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tok.tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "trainer.train()\n",
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69746632-d240-44ae-bcf6-5964015a5402",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         'V'       0.00      0.00      0.00         0\n",
      "         '_'       0.99      0.99      0.99     95833\n",
      "       'C-V'       0.00      0.00      0.00         0\n",
      "      'ARGA'       0.00      0.00      0.00         0\n",
      "      'ARG3'       0.00      0.00      0.00        82\n",
      "      'ARG2'       0.73      0.75      0.74      1298\n",
      "      'ARG5'       0.00      0.00      0.00         1\n",
      "      'ARG0'       0.81      0.86      0.83      1389\n",
      "      'ARG4'       0.63      0.48      0.55        54\n",
      "      'ARG1'       0.82      0.86      0.84      3218\n",
      "    'C-ARG4'       0.00      0.00      0.00         0\n",
      "    'C-ARG2'       0.00      0.00      0.00         7\n",
      "    'R-ARG2'       0.00      0.00      0.00         4\n",
      "    'C-ARG0'       0.00      0.00      0.00         5\n",
      "    'R-ARG0'       0.76      0.87      0.81        54\n",
      "    'C-ARG1'       0.43      0.27      0.33        55\n",
      "    'C-ARG3'       0.00      0.00      0.00         7\n",
      "    'R-ARG3'       0.00      0.00      0.00         1\n",
      "    'R-ARG1'       0.72      0.71      0.71        65\n",
      "  'ARGM-EXT'       0.80      0.75      0.78       114\n",
      "  'ARGM-NEG'       0.90      0.93      0.92       305\n",
      "  'ARGM-DIS'       0.77      0.58      0.66        97\n",
      "  'ARGM-PRD'       0.71      0.09      0.17        53\n",
      "  'ARGM-PRP'       0.42      0.62      0.50        63\n",
      "  'ARGM-ADV'       0.68      0.60      0.64       454\n",
      "  'ARGM-GOL'       0.00      0.00      0.00        28\n",
      "  'ARGM-REC'       0.00      0.00      0.00         4\n",
      "  'ARG1-DSP'       0.00      0.00      0.00         0\n",
      "  'ARGM-MNR'       0.51      0.54      0.53       169\n",
      "  'ARGM-PRR'       0.67      0.73      0.70        77\n",
      "  'ARGM-DIR'       0.41      0.40      0.41        52\n",
      "  'ARGM-LVB'       0.72      0.76      0.74        66\n",
      "  'ARGM-TMP'       0.77      0.85      0.81       540\n",
      "  'ARGM-ADJ'       0.70      0.73      0.71       235\n",
      "  'ARGM-MOD'       0.93      0.96      0.95       368\n",
      "  'ARGM-COM'       0.00      0.00      0.00        15\n",
      "  'ARGM-LOC'       0.52      0.62      0.57       276\n",
      "  'ARGM-CAU'       0.43      0.47      0.45        62\n",
      "  'ARGM-CXN'       1.00      0.14      0.25        14\n",
      "'C-ARG1-DSP'       0.00      0.00      0.00         0\n",
      "'R-ARGM-DIR'       0.00      0.00      0.00         0\n",
      "'R-ARGM-LOC'       0.50      0.20      0.29        10\n",
      "'R-ARGM-ADV'       0.00      0.00      0.00         1\n",
      "'R-ARGM-MNR'       0.00      0.00      0.00         2\n",
      "'C-ARGM-EXT'       0.00      0.00      0.00         2\n",
      "'C-ARGM-LOC'       0.00      0.00      0.00         3\n",
      "'C-ARGM-CXN'       0.00      0.00      0.00         8\n",
      "'R-ARGM-ADJ'       0.00      0.00      0.00         0\n",
      "'R-ARGM-TMP'       0.00      0.00      0.00         8\n",
      "'R-ARGM-CAU'       0.00      0.00      0.00         1\n",
      "'C-ARGM-MNR'       0.00      0.00      0.00         1\n",
      "'R-ARGM-COM'       0.00      0.00      0.00         1\n",
      "\n",
      "   micro avg       0.97      0.97      0.97    105102\n",
      "   macro avg       0.33      0.30      0.30    105102\n",
      "weighted avg       0.97      0.97      0.97    105102\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions_raw, labels, _ = trainer.predict(tokenized_datasets[\"validation\"])\n",
    "predictions = np.argmax(predictions_raw, axis=2)\n",
    "\n",
    "list_predictions = [\n",
    "    [labels_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "    for prediction, label in zip(predictions, labels)\n",
    "]\n",
    "true_labels = [\n",
    "    [labels_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "    for prediction, label in zip(predictions, labels)\n",
    "]\n",
    "\n",
    "results = metric.compute(predictions=list_predictions, references=true_labels)\n",
    "\n",
    "val_word_ids = []\n",
    "for sentence in dataset['validation']['sentence']:\n",
    "    val_word_ids.append(tok.tokenizer(sentence, truncation=True, is_split_into_words=True).word_ids())\n",
    "\n",
    "df = pd.DataFrame(columns=['sentence', 'prediction', 'gold', 'word_ids'])\n",
    "for tokens, prediction, gold, word_ids in zip(tokenized_datasets['validation']['input_ids'], list_predictions, true_labels, val_word_ids):\n",
    "    sentence = tok.tokenizer.decode(tokens)\n",
    "    df.loc[len(df.index)] = [sentence, prediction, gold, word_ids]\n",
    "\n",
    "gold_restored = []\n",
    "pred_restored = []\n",
    "for i, row in df.iterrows():\n",
    "    sentence = row[0]\n",
    "    orig_sentence = sentence.split('[SEP]')[0].split(' ')[1:]\n",
    "    prediction = row[1]\n",
    "    gold = row[2]\n",
    "    word_ids = row[3][1:-1]\n",
    "    gold_restored.append(shrink_predictions(word_ids, gold))\n",
    "    pred_restored.append(shrink_predictions(word_ids, prediction))\n",
    "\n",
    "df['gold_restored'] = gold_restored\n",
    "df['pred_restored'] = pred_restored\n",
    "df.to_csv('data/output/context_val.csv')\n",
    "\n",
    "class_report_base('data/output/context_val.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88abcbb-32d6-4f9c-8995-16dc67dc6717",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
