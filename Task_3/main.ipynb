{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n",
        "from transformers import DataCollatorForTokenClassification"
      ],
      "metadata": {
        "id": "U6bQle4zH0TW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4pnFGHsQGew_"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def _get_predicates_from_sentence(lines):\n",
        "    # here we expect that sentence is a number of lines\n",
        "    pred_positions = []\n",
        "    pred_columns = []\n",
        "\n",
        "    for line in lines[2:]:\n",
        "        split_line = line.split('\\t')\n",
        "        if any([c == 'V' for c in split_line[11:]]):\n",
        "            pred_index = split_line.index('V')\n",
        "            if not split_line[0].isdigit():\n",
        "                print('found shit:', split_line[0:1])\n",
        "            else:\n",
        "                pred_positions.append(int(split_line[0])-1)\n",
        "                pred_columns.append(pred_index)\n",
        "    return pred_positions, pred_columns\n",
        "\n",
        "\n",
        "\n",
        "def process_file(conll_file)->pd.DataFrame:\n",
        "    big_df = pd.DataFrame(columns=['sentence', 'predicate', 'pred columns', 'labels'])\n",
        "    with open(conll_file) as f:\n",
        "        text = f.read()\n",
        "    sentences = text.split('\\n\\n')  # split by empty line - sent id+text+table with features\n",
        "    for s in sentences:\n",
        "        lines = s.split('\\n')\n",
        "        if lines[0].startswith('# propbank'):\n",
        "            lines = lines[1:]\n",
        "        if lines[0].startswith('# newdoc'):\n",
        "            lines = lines[1:]\n",
        "\n",
        "\n",
        "        if len(lines) > 1:\n",
        "            sentence_words_list = [l.split('\\t')[1] for l in lines[2:]]\n",
        "            pred_idxs, pred_cols = _get_predicates_from_sentence(lines)\n",
        "\n",
        "            labels = find_tokens_args(lines, pred_cols)\n",
        "\n",
        "            for idx, col, label in zip(pred_idxs, pred_cols, labels):\n",
        "                word = lines[idx+2].split('\\t')[1]\n",
        "                big_df.loc[len(big_df.index)] = [sentence_words_list, word, col, ', '.join(label)]\n",
        "\n",
        "    print('process_file(): dataframe len:', len(big_df))\n",
        "    return big_df\n",
        "\n",
        "\n",
        "def _get_context_of_predicate(sentence_words_list, word, idx):\n",
        "    context = ['_', word, '_']\n",
        "\n",
        "    if idx >= 1:\n",
        "        context[0] = sentence_words_list[idx-1]\n",
        "\n",
        "    if idx < len(sentence_words_list)-1:\n",
        "        context[2] = sentence_words_list[idx+1]\n",
        "\n",
        "    return context\n",
        "\n",
        "\n",
        "def advanced_process_file(conll_file)->pd.DataFrame:\n",
        "    big_df = pd.DataFrame(columns=['sentence', 'predicate', 'pred columns', 'context', 'labels'])\n",
        "    with open(conll_file) as f:\n",
        "        text = f.read()\n",
        "    sentences = text.split('\\n\\n')  # split by empty line - sent id+text+table with features\n",
        "    for s in sentences:\n",
        "        lines = s.split('\\n')\n",
        "        if lines[0].startswith('# propbank'):\n",
        "            lines = lines[1:]\n",
        "        if lines[0].startswith('# newdoc'):\n",
        "            lines = lines[1:]\n",
        "        if len(lines) > 1:\n",
        "            sentence_words_list = [l.split('\\t')[1] for l in lines[2:]]\n",
        "            pred_idxs, pred_cols = _get_predicates_from_sentence(lines)\n",
        "            labels = find_tokens_args(lines, pred_cols)\n",
        "            for idx, col, label in zip(pred_idxs, pred_cols, labels):\n",
        "                word = lines[idx+2].split('\\t')[1]\n",
        "                context = _get_context_of_predicate(sentence_words_list, word, idx)\n",
        "                big_df.loc[len(big_df.index)] = [sentence_words_list, word, col, context, ', '.join(label)]\n",
        "\n",
        "    print('advanced_process_file(): dataframe len:', len(big_df))\n",
        "    return big_df\n",
        "\n",
        "\n",
        "def find_tokens_args(lines, pred_cols):\n",
        "    labels = []\n",
        "    for i, predicate_col in enumerate(pred_cols):\n",
        "        labels.append([])\n",
        "        for line in lines[2:]:\n",
        "            tags = line.split('\\t')\n",
        "            try:\n",
        "                label = tags[predicate_col]\n",
        "                if label == '':\n",
        "                    label = '_'\n",
        "                labels[i].append(label)\n",
        "            except:\n",
        "                pass\n",
        "    return labels\n",
        "\n",
        "\n",
        "def extract_features(dataframe)->pd.DataFrame:\n",
        "    raise ValueError(\"do not use this method!!\")\n",
        "    df = pd.DataFrame(columns=['sentences', 'labels', 'labels_list'])\n",
        "    df.sentences = [a + ['[SEP]', b] for a, b in zip(dataframe['sentence'].values, dataframe['predicate'].values)]\n",
        "    # for now I put here the index of word, but it should be the label we predict\n",
        "    df.labels = dataframe['pred columns values']\n",
        "    df.labels_list = [l.split(', ') for l in df.labels]\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U5h4T1POGexD"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from datasets import Dataset, DatasetDict, load_metric\n",
        "import transformers\n",
        "from transformers import AutoTokenizer\n",
        "import numpy as np\n",
        "\n",
        "task = \"ner\"\n",
        "model_checkpoint = \"distilbert-base-uncased\"\n",
        "labels_list = None\n",
        "metric = load_metric(\"seqeval\")\n",
        "\n",
        "batch_size = 32 # subject to change, the bigger the better, but should fit into memory\n",
        "\n",
        "def convert_to_dataset(train:pd.DataFrame,\n",
        "                       val:pd.DataFrame,\n",
        "                       test:pd.DataFrame)->DatasetDict:\n",
        "    global labels_list\n",
        "    train_ds = Dataset.from_pandas(train)\n",
        "    val_ds = Dataset.from_pandas(val)\n",
        "    test_ds = Dataset.from_pandas(test)\n",
        "\n",
        "    ds = DatasetDict()\n",
        "\n",
        "    ds['train'] = train_ds\n",
        "    ds['validation'] = val_ds\n",
        "    ds['test'] = test_ds\n",
        "\n",
        "    if not labels_list:\n",
        "        labels_list = get_labels_list_from_dataset(ds)\n",
        "\n",
        "    return ds\n",
        "\n",
        "\n",
        "def get_labels_list_from_dataset(ds:DatasetDict):\n",
        "    labels_set = set()\n",
        "\n",
        "    for ds_name in ['train', 'test', 'validation']:\n",
        "        for label in ds[ds_name]['labels']:\n",
        "            vals = label.split(', ')\n",
        "            for v in vals:\n",
        "                labels_set.add(v)\n",
        "    labels_list = list(labels_set)\n",
        "    return labels_list\n",
        "\n",
        "\n",
        "class Tokenizer:\n",
        "    def __init__(self, model_checkpoint, labels_list) -> None:\n",
        "        \"\"\"\n",
        "        :param model_checkpoint\n",
        "        \"\"\"\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "        assert isinstance(self.tokenizer, transformers.PreTrainedTokenizerFast), \"tokenizer is not PreTrainedTokenizerFast!\"\n",
        "\n",
        "        self.labels_list = labels_list\n",
        "\n",
        "\n",
        "    def _tokenize_input_string(self, input):\n",
        "        if isinstance(input, str):\n",
        "            return self.tokenizer(input, truncation=True)\n",
        "        elif isinstance(input, list):\n",
        "            return self.tokenizer(input, truncation=True, is_split_into_words=True)\n",
        "        else:\n",
        "            raise TypeError(f'tokenizer input should be str or list, got {type(input)}')\n",
        "\n",
        "\n",
        "    def tokenize_align_labels_no_pred(self, examples):\n",
        "        global labels_list\n",
        "        tokenized_sentences = self.tokenizer(examples[\"sentence\"], truncation=True, is_split_into_words=True)\n",
        "\n",
        "        list_of_labels_list = [l.split(', ') for l in examples['labels']]\n",
        "\n",
        "        labels_out = []\n",
        "        for i, (sentence, labels_as_list) in enumerate(zip(examples['sentence'], list_of_labels_list)):\n",
        "            tokenized_sentence = self.tokenizer(sentence, truncation=True, is_split_into_words=True)\n",
        "            labels = []\n",
        "            for word_id in tokenized_sentence.word_ids():\n",
        "                try:\n",
        "                    labels.append(-100 if word_id is None else labels_list.index(labels_as_list[word_id]))\n",
        "                except:\n",
        "                    labels.append(labels_list.index('_')) # for specific example with 28 words and 27 labels\n",
        "            labels_out.append(labels)\n",
        "\n",
        "        tokenized_sentences['labels'] = labels_out\n",
        "\n",
        "        return tokenized_sentences\n",
        "\n",
        "\n",
        "    def tokenize_and_align_labels_pred(self, examples):\n",
        "        global labels_list\n",
        "        tokenized_sentences = self.tokenizer(examples[\"sentence\"], truncation=False, is_split_into_words=True)\n",
        "        tokenized_predicates = self.tokenizer(examples[\"predicate\"], truncation=False, is_split_into_words=False)\n",
        "\n",
        "        tokenized_inputs = dict()\n",
        "        for key in tokenized_sentences.keys():\n",
        "            tokenized_inputs[key] = [v1 + v2[1:] for v1, v2 in zip(tokenized_sentences[key], tokenized_predicates[key])]\n",
        "\n",
        "        list_of_labels_list = [l.split(', ') for l in examples['labels']]\n",
        "\n",
        "        labels_out = []\n",
        "        for i, (sentence, predicate, labels_as_list) in enumerate(zip(examples['sentence'], examples['predicate'], list_of_labels_list)):\n",
        "            # sentence = ex['sentence']\n",
        "            tokenized_sentence = self.tokenizer(sentence, truncation=True, is_split_into_words=True)\n",
        "            labels = []\n",
        "            pred_position = sentence.index(predicate)\n",
        "            for word_id in tokenized_sentence.word_ids():\n",
        "                try:\n",
        "                    labels.append(-100 if word_id is None else labels_list.index(labels_as_list[word_id]))\n",
        "                except:\n",
        "                    labels.append(labels_list.index('_')) # for specific example with 28 words and 27 labels\n",
        "\n",
        "            count = tokenized_sentence.word_ids().count(pred_position)\n",
        "\n",
        "            labels += [labels_list.index('_')]*count\n",
        "            labels.append(-100)\n",
        "\n",
        "            labels_out.append(labels)\n",
        "\n",
        "        tokenized_inputs['labels'] = labels_out\n",
        "\n",
        "        return tokenized_inputs\n",
        "\n",
        "    def tokenize_and_align_labels_context(self, examples):\n",
        "        global labels_list\n",
        "        tokenized_sentences = self.tokenizer(examples[\"sentence\"], truncation=False, is_split_into_words=True)\n",
        "        tokenized_context = self.tokenizer(examples[\"context\"], truncation=False, is_split_into_words=True)\n",
        "\n",
        "        tokenized_inputs = dict()\n",
        "        for key in tokenized_sentences.keys():\n",
        "            tokenized_inputs[key] = [v1 + v2[1:] for v1, v2 in zip(tokenized_sentences[key], tokenized_context[key])]\n",
        "\n",
        "        list_of_labels_list = [l.split(', ') for l in examples['labels']]\n",
        "\n",
        "        labels_out = []\n",
        "        for i, (sentence, context, labels_as_list) in enumerate(zip(examples['sentence'], examples['context'], list_of_labels_list)):\n",
        "            tokenized_sentence = self.tokenizer(sentence, truncation=True, is_split_into_words=True)\n",
        "            labels = []\n",
        "            pred_positions = [sentence.index(c) if c!='_' else -1 for c in context]\n",
        "            word_ids = tokenized_sentence.word_ids()\n",
        "            for word_id in word_ids:\n",
        "                try:\n",
        "                    labels.append(-100 if word_id is None else labels_list.index(labels_as_list[word_id]))\n",
        "                except:\n",
        "                    labels.append(labels_list.index('_')) # for specific example with 28 words and 27 labels\n",
        "\n",
        "\n",
        "            base_count = len(self.tokenizer('_')['input_ids'])-2\n",
        "            for pred_position in pred_positions:\n",
        "                if pred_position == -1:\n",
        "                    labels += [labels_list.index('_')]*base_count\n",
        "                    continue\n",
        "                count = word_ids.count(pred_position)\n",
        "                labels += [labels_list.index(labels_as_list[pred_position])]*count\n",
        "            labels.append(-100)\n",
        "\n",
        "            labels_out.append(labels)\n",
        "\n",
        "        tokenized_inputs['labels'] = labels_out\n",
        "        return tokenized_inputs\n",
        "\n",
        "def compute_metrics(p):\n",
        "    predictions, labels = p\n",
        "    predictions = np.argmax(predictions, axis=2)\n",
        "\n",
        "    # Remove ignored index (special tokens)\n",
        "    true_predictions = [\n",
        "        [labels_list[p] for (p, l) in zip(prediction, label) if l != -100 and p < len(labels_list)]\n",
        "        for prediction, label in zip(predictions, labels)\n",
        "    ]\n",
        "    true_labels = [\n",
        "        [labels_list[l] for (p, l) in zip(prediction, label) if l != -100 and p < len(labels_list)]\n",
        "        for prediction, label in zip(predictions, labels)\n",
        "    ]\n",
        "\n",
        "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
        "    return {\n",
        "        \"precision\": results[\"overall_precision\"],\n",
        "        \"recall\": results[\"overall_recall\"],\n",
        "        \"f1\": results[\"overall_f1\"],\n",
        "        \"accuracy\": results[\"overall_accuracy\"],\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ErB26AmPGexG"
      },
      "outputs": [],
      "source": [
        "task = \"ner\" # Should be one of \"ner\", \"pos\" or \"chunk\"\n",
        "model_checkpoint = \"distilbert-base-uncased\"\n",
        "batch_size = 32"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WUvyO_ZfGexG"
      },
      "outputs": [],
      "source": [
        "df_val = advanced_process_file('/content/en_ewt-up-dev.conllu')\n",
        "df_train = advanced_process_file('/content/en_ewt-up-train.conllu')\n",
        "df_test = advanced_process_file('/content/en_ewt-up-test.conllu')\n",
        "\n",
        "# df_val = process_file('/content/en_ewt-up-dev.conllu')\n",
        "# df_train = process_file('/content/en_ewt-up-train.conllu')\n",
        "# df_test = process_file('/content/en_ewt-up-test.conllu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fGeIlh_RGexI"
      },
      "outputs": [],
      "source": [
        "dataset = convert_to_dataset(df_train, df_val, df_test)\n",
        "labels_list = get_labels_list_from_dataset(dataset)\n",
        "print(sorted(labels_list))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oO33ERDPGexI"
      },
      "outputs": [],
      "source": [
        "tok = Tokenizer(model_checkpoint, labels_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qqy-HHS2GexJ"
      },
      "outputs": [],
      "source": [
        "# tokenized_datasets = dataset.map(tok.tokenize_and_align_labels_context, batched=True)\n",
        "tokenized_datasets = dataset.map(tok.tokenize_and_align_labels_pred, batched=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z0bf8otVGexJ"
      },
      "outputs": [],
      "source": [
        "# model = AutoModelForTokenClassification.from_pretrained(model_checkpoint, num_labels=len(labels_list))\n",
        "model = AutoModelForTokenClassification.from_pretrained('/content/model_checkpoints/pred', num_labels=len(labels_list))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IZgrSP1xGexK"
      },
      "outputs": [],
      "source": [
        "model_name = model_checkpoint.split(\"/\")[-1]\n",
        "args = TrainingArguments(\n",
        "    f\"model_checkpoints/pred\",\n",
        "    evaluation_strategy = 'epoch',\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    num_train_epochs=1,\n",
        "    weight_decay=0.01,\n",
        "    push_to_hub=False,\n",
        ")\n",
        "\n",
        "data_collator = DataCollatorForTokenClassification(tok.tokenizer)\n",
        "metric = load_metric(\"seqeval\")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model,\n",
        "    args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation\"],\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tok.tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i, (s, l) in enumerate(zip(tokenized_datasets['train']['input_ids'], tokenized_datasets['train']['labels'])):\n",
        "    if len(s) != len(l):\n",
        "        print(f'{i}: s:{len(s)}, l:{len(l)}')"
      ],
      "metadata": {
        "id": "o9K9-0gdNPfL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "veXQ31mxGexL"
      },
      "outputs": [],
      "source": [
        "# trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.evaluate()"
      ],
      "metadata": {
        "id": "VX7gKfH0IbBZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HexqnT2uGexN"
      },
      "outputs": [],
      "source": [
        "predictions_raw, labels, _ = trainer.predict(tokenized_datasets[\"validation\"])\n",
        "predictions = np.argmax(predictions_raw, axis=2)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "true_predictions = [\n",
        "    [labels_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
        "    for prediction, label in zip(predictions, labels)\n",
        "]\n",
        "true_labels = [\n",
        "    [labels_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
        "    for prediction, label in zip(predictions, labels)\n",
        "]\n",
        "\n",
        "results = metric.compute(predictions=true_predictions, references=true_labels)\n",
        "results"
      ],
      "metadata": {
        "collapsed": true,
        "id": "xQldgMFYKson"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# trainer.save_model()"
      ],
      "metadata": {
        "id": "-9gdt6tyLIk_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LDcH_UtiXi3G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(columns=['sentence', 'prediction', 'gold'])\n",
        "for tokens, prediction, gold in zip(tokenized_datasets['validation']['input_ids'], true_predictions, true_labels):\n",
        "    sentence = tok.tokenizer.decode(tokens)\n",
        "    df.loc[len(df.index)] = [sentence, prediction, gold]"
      ],
      "metadata": {
        "id": "nXQLIwAcLvKT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv('base.csv')"
      ],
      "metadata": {
        "id": "7mTV5Y1gWmlf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "-8B_RWEVYwOe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "498zYjPvZvUC"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "21634ba1c762bc3505d86a0c5bab9f12f9ed8aa21c75b1f37c314fb51f344c98"
      }
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}